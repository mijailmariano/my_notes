{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module: Natural Language Processing\n",
    "\n",
    "    Created By: Mijail Q. Mariano\n",
    "\n",
    "    290900AUGUST2022\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``29AUG22 Notes:``\n",
    "\n",
    "\n",
    "- NLP is bringing our programming and 'algorithmic chops' to bear on understanding natural language\n",
    "\n",
    "- NLP is an enture branch of Computer Science / Data Science, it's not one or two algorithms\n",
    "\n",
    "- Examples of NLP project/tool/or thing:\n",
    "    - autocorrect\n",
    "    - search engines\n",
    "    - automated helpdesk\n",
    "    - virtual textbots\n",
    "    - email filtering/spam detection\n",
    "    - sentiment analysis\n",
    "    - topic modeling\n",
    "    - text normalization \n",
    "\n",
    "**Topics that will be covered:**\n",
    "\n",
    "- Regular Expression (kind of like a word/letter algebra for searching/replacing/filtering)\n",
    "- Web scraping\n",
    "- Exploration (e.g., wordclouds)\n",
    "- Classification Modeling\n",
    "- NLP project will classify the programming language of a repo off of GitHUB based only on that repo's README file\n",
    "\n",
    "**<u>Module Key Words:</u>**\n",
    "\n",
    "- Chunking\n",
    "- Topic Modeling\n",
    "- Text Pre-processing\n",
    "- Text Representation\n",
    "- \"Corpus\" of Text = Body of Text\n",
    "- Vectorization\n",
    "- Bag of Words = frequency count of words\n",
    "- Tokerization\n",
    "- Document = hierachy of text (e.g., text, poems, emails, novels, textbooks, etc.)\n",
    "\n",
    "-----\n",
    "\n",
    "**``Ryan Orsinger Notes:``**\n",
    "\n",
    "NLP - Natural Language Processing\n",
    "Anticipate a group project\n",
    "\n",
    "**\"Natural Language\"** = human language\n",
    "- syntax can be very fluid\n",
    "- lots of meaning through context\n",
    "\n",
    "**\"Formal Language\"**\n",
    "- a formal language is a grammer/syntax\n",
    "- it's a language that's very rigid\n",
    "- formal language is so rigid, that it's predicable.\n",
    "- **Examples:**\n",
    "    - math notation\n",
    "    - chemistry notation\n",
    "    - musical notation\n",
    "    - logical notation\n",
    "    - programming languages\n",
    "        - Python\n",
    "        - SQL\n",
    "        - C, C#, C++, Objective C\n",
    "        - ANY programming language\n",
    "\n",
    "\n",
    "- NLP is bringing our programming and algorithmic chops to bear on understanding natural language\n",
    "- NLP is an entire branch of CS/DS, it's not one or two algorithms\n",
    "\n",
    "**``Examples of NLP project/tool/thing:``**\n",
    "- Siri, google assitant\n",
    "- voice to text (machine listening)\n",
    "- language translation tools (from Spanish to French..)\n",
    "- text to voice \n",
    "- automated helpdesk - Interactive Voice Response\n",
    "- autocorrect and text prediction\n",
    "- virtual chatbots (text responses/generation)\n",
    "- web scraping bots \n",
    "- search engines\n",
    "- regular language to regular expression tool\n",
    "- hashtag, topic search #running but you search for #run and you see the thread b/c they share a root\n",
    "- email filters - searching\n",
    "- spam detection\n",
    "- accessibility tools for hearing/sight impairment\n",
    "- sentiment analysis (what's the tone of a document, is this a document of facts or opinions or positive/negative)\n",
    "- topic modeling (unsupervised algorithm, trying to figure out what a document is all about)\n",
    "- parts of speech tagging\n",
    "- text normalization (removing as much variance from text without losing too much meaning - lowercasing everything)\n",
    "\n",
    "**``Topics we'll cover``**\n",
    "- Regular expressions (kinda like a word/letter algebra for searching/replacing/filtering)\n",
    "- Web scraping\n",
    "- We'll do some exploration (this could be its own 6 month course)\n",
    "- classification modeling \n",
    "- NLP project is classifying the programming language of a repo off of GitHub based only on that repo's README file.\n",
    "\n",
    "**``End Goal for y'all w/ this module``**\n",
    "1. Enough regex to be dangerous (regex is a tool for your toolbox)\n",
    "2. Enough web scraping and html parsing to be useful (to build your own datasets)\n",
    "3. Modeling goal is to build your own dataset and perform classification on that labeled dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**``Regular Expression or \"regex\" or pythons library \"re\"``**\n",
    "\n",
    "- Regular expressions (called regexes or regex patterns) are a tiny language for dealing with text and character patterns.\n",
    "- With RegEx patterns we can:\n",
    "    - Does this string match a pattern?\n",
    "    - Is there a match for the pattern anywhere in the string?\n",
    "    - Modify + split strings in various ways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **re library functions**\n",
    "\n",
    "- `re.search` scans through a string, looking for any location where the RE matches.\n",
    "- `re.findall` Finds all substrings where the RE matches; returns a list.\n",
    "- `re.split` splits a string on a given regex pattern, removing that pattern. The result is a list of a strings.\n",
    "- `re.sub` allows us to match a regex and substitute in a new substring for the match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**``\"r\" in front of quotes means \"raw string\"``**\n",
    "\n",
    "**for example:**\n",
    "\n",
    "re.findall(r\"'b', 'abcd'\")\n",
    "\n",
    "<br>\n",
    "\n",
    "**``\"Flags\"``**\n",
    "\n",
    "- Are argument type expressions that allows you to manipulate or bypass default settings (e.g., \"re.IGNORECASE\")\n",
    "\n",
    "**or expressed as:**   re.search(r\"two\", string, re.IGNORECASE)\n",
    "\n",
    "<br>\n",
    "\n",
    "**``Pike characters '|' represent 'or'``**\n",
    "\n",
    "**for example:**\n",
    "\n",
    "re.search(f\"grey|gray\", \"I can't remember if you spell grey gray or gray as grey.\")\n",
    "\n",
    "\n",
    "#### ``\"Greedy Matching\"``\n",
    "\n",
    "(remember) `\\w` matches any alphanumeric character. Equivalent to `[a-zA-Z0-9_]`\n",
    "\n",
    "(remember) `\\W` matches any non-alphanumeric character. Equivalent to `[^a-zA-Z0-9_]`\n",
    "\n",
    "* .* finds the largest possible match\n",
    "* technical term is greedy\n",
    "* re.search(r\"^b.*\", \"bananarama pajama\")\n",
    "\n",
    "\n",
    "**<u>HTTP verbs:</u>**\n",
    "\n",
    "* \"GET\"\n",
    "* \"POST\"\n",
    "\n",
    "``User Agent Strings`` refers to string that identifies/detects devices like mobile phones, tablets and their capabilities by parsing (browser/HTTP) user agent strings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Text \"preparation\" involves:``\n",
    "\n",
    "* Convert text to all lower case for normalcy.\n",
    "* Remove any accented characters, non-ASCII characters.\n",
    "* Remove special characters.\n",
    "* Stem or lemmatize the words.\n",
    "* Remove stopwords.\n",
    "* Store the clean text and the original text for use in future notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK = \"natural language toolkit\"\n",
    "# NLTK imports \n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "    date: Thursday, September 01 2022\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**``Feature Extraction: TF-IDF``**\n",
    "\n",
    "Terms:\n",
    "\n",
    "* Term Frequency (TF): Term Frequency; how often a word appears in a document\n",
    "* IDF: Inverse Documnet Frequency; a measure based on in how many documents will a word appear\n",
    "* TF-IDF: A combination of the two measures above\n",
    "\n",
    "**``Term Frequency (TF)``**\n",
    "\n",
    "* Raw Count: This is simply the count of the number of occurances of each word.\n",
    "* Frequency: The number of times each word appears divided by the total number of words.\n",
    "* Augmented Frequency: The frequency of each word divided by the maximum frequency. This can help prevent bias towards larger documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
